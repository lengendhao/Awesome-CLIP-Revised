# Awesome-CLIP-Revised
Awesome updated list for research on CLIP (Contrastive Language-Image Pre-Training). 

The [Awesome-CLIP](https://github.com/yzhuoning/Awesome-CLIP) hasn't been updated for a long time, so I make this repo and revise the list.

## CLIP
- Learning Transferable Visual Models From Natural Language Supervision, 2021.03 [[paper]](https://arxiv.org/pdf/2103.00020.pdf) [[code]](https://github.com/openai/CLIP)
- CLIP: Connecting Text and Image [[web]](https://openai.com/research/clip)
- Multimodal Neurons in Artificial Neural Networks [[web]](https://openai.com/blog/multimodal-neurons/)

## Code
- OpenCLIP (PyTorch) [[code](https://github.com/mlfoundations/open_clip)]  
- Train-CLIP (PyTorch) [[code](https://github.com/Zasder3/train-CLIP)] 
- Paddle-CLIP (PaddlePaddle) [[code](https://github.com/AgentMaker/Paddle-CLIP)] 

## CLIP for Various tasks
### Text-to-Image Generation
- Big Sleep: A simple command line tool for text to image generation [[code]](https://github.com/lucidrains/big-sleep)
- Deep Daze: A simple command line tool for text to image generation [[code]](https://github.com/lucidrains/deep-daze)
- VQGAN-CLIP [[code]](https://github.com/nerdyrodent/VQGAN-CLIP)
- CLIP Guided Diffusion [[code]](https://github.com/afiaka87/clip-guided-diffusion)
- StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery, 2021.03 [[paper]](https://arxiv.org/pdf/2103.17249.pdf) [[code]](https://github.com/orpatashnik/StyleCLIP)
- TargetCLIP: Image-Based CLIP-Guided Essence Transfer, 2021.10 [[paper]](https://arxiv.org/pdf/2110.12427.pdf) [[code]](https://github.innominds.com/hila-chefer/TargetCLIP)
- DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation, 2021.10 [[paper]](https://arxiv.org/pdf/2110.02711.pdf) [[code]](https://github.com/gwang-kim/DiffusionCLIP)
- CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions, 2021.12 [[paper]](https://arxiv.org/pdf/2112.05219.pdf) [[code]](https://github.com/RameenAbdal/CLIP2StyleGAN)
- CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP, 2022.03 [[paper]](https://arxiv.org/pdf/2203.00386.pdf) [[code]](https://github.com/HFAiLab/clip-gen)
- CLIP-CLOP: CLIP-Guided Collage and Photomontage, 2022.05 [[paper]](https://arxiv.org/pdf/2205.03146v2.pdf) [[code]](https://github.com/deepmind/arnheim)
- Clip2latent: Text driven sampling of a pre-trained StyleGAN using denoising diffusion and CLIP, 2022.10 [[paper]](https://arxiv.org/pdf/2210.02347.pdf) [[code]](https://github.com/justinpinkney/clip2latent)

### Object Detection
- Roboflow Zero-shot Object Tracking [[code]](https://github.com/roboflow-ai/zero-shot-object-tracking)
- Crop-CLIP [[code]](https://github.com/vijishmadhavan/Crop-CLIP)
- Detic: Detecting Twenty-thousand Classes using Image-level Supervision, 2022.01 [[paper]](https://arxiv.org/pdf/2201.02605.pdf) [[code]](https://github.com/facebookresearch/Detic)
- ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension, 2022.04 [[paper]](https://arxiv.org/pdf/2204.05991.pdf) [[code]](https://github.com/allenai/reclip)  

### Information Retrieval
- Unsplash Image Search [[code]](https://github.com/haltakov/natural-language-image-search)
- Natural Language YouTube Search [[code](https://github.com/haltakov/natural-language-youtube-search)]
- CLIP-as-service: Embed images and sentences into fixed-length vectors with CLIP [[code]](https://github.com/jina-ai/clip-as-service)
- clip-retrieval [[code](https://github.com/rom1504/clip-retrieval)]
- CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval, 2021.04 [[paper]](https://arxiv.org/pdf/2104.08860.pdf) [[code]](https://github.com/ArrowLuo/CLIP4Clip)
- CLIP2Video: Mastering Video-Text Retrieval via Image CLIP, 2021.06 [[paper]](https://arxiv.org/pdf/2106.11097.pdf) [[code]](https://github.com/CryhanFang/CLIP2Video)
- A CLIP-Hitchhiker’s Guide to Long Video Retrieval, 2022.05 [[paper]](https://arxiv.org/pdf/2205.08508.pdf)
- X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval, 2022.07 [[paper]](https://arxiv.org/pdf/2207.07285.pdf) [[code]](https://github.com/xuguohai/X-CLIP)
- Extending CLIP for Category-to-image Retrieval in E-commerce, 2022 [[paper]](https://mariyahendriksen.github.io/files/ecir22.pdf) [[code]](https://github.com/mariyahendriksen/ecir2022_category_to_image_retrieval)

### Representation Learning
- Multilingual-CLIP [[code]](https://github.com/FreddeFrallan/Multilingual-CLIP)
- CLIP (With Haiku + Jax!) [[code]](https://github.com/kingoflolz/CLIP_JAX)
- Wav2CLIP: Learning Robust Audio Representations From CLIP, 2021.10 [[paper]](https://arxiv.org/pdf/2110.11499.pdf) [[code]](https://github.com/descriptinc/lyrebird-Wav2CLIP)
- Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm, 2021.10 [[paper]](https://arxiv.org/pdf/2110.05208.pdf) [[code]](https://github.com/Sense-GVT/DeCLIP)
- SLIP: Self-supervision meets Language-Image Pre-training, 2021.12 [[paper]](https://arxiv.org/pdf/2112.12750.pdf) [[code]](https://github.com/facebookresearch/SLIP)
- CLIP-Lite: Information Efficient Visual Representation Learning from Textual Annotation, 2021.12 [[paper]](https://arxiv.org/pdf/2112.07133.pdf)
- RegionCLIP: Region-based Language-Image Pretraining, 2021.12 [[paper]](https://arxiv.org/pdf/2112.09106.pdf) [[code]](https://github.com/microsoft/RegionCLIP)
- CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification, 2021.12 [[paper]](https://arxiv.org/pdf/2112.03562.pdf)
- DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting, 2021.12 [[paper]](https://arxiv.org/pdf/2112.01518.pdf) [[code]](https://github.com/raoyongming/DenseCLIP)
- Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision, 2022.03 [[paper]](https://arxiv.org/pdf/2203.05796v1.pdf) [[code]](https://github.com/sense-gvt/declip)
- PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining, 2022.04 [[paper]](https://arxiv.org/pdf/2204.14095v2.pdf) [[code]](https://github.com/Yuting-Gao/PyramidCLIP)
- CyCLIP: Cyclic Contrastive Language-Image Pretraining, 2022.05 [[paper]](https://arxiv.org/pdf/2205.14459v1.pdf) [[code]](https://github.com/goel-shashank/CyCLIP)
- Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training, 2022.07 [[paper]](https://arxiv.org/pdf/2207.12661.pdf) [[code]](https://github.com/Hxyou/MSCLIP)
- MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining, 2022.08 [[paper]](https://arxiv.org/pdf/2208.12262.pdf)
- CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment, 2022.09 [[paper]](https://arxiv.org/pdf/2209.06430.pdf) [[code]](https://github.com/microsoft/XPretrain/tree/main/CLIP-ViP)
- DetCLIP: Dictionary-Enriched Visual-Concept Paralleled Pre-training for Open-world Detection, 2022.09 [[paper]](https://arxiv.org/pdf/2209.09407.pdf) [[code]](https://github.com/Sense-GVT/DeCLIP)
- UniCLIP: Unified Framework for Contrastive Language–Image Pre-training, 2022.09 [[paper]](https://arxiv.org/pdf/2209.13430.pdf)
- SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model, 2022.10 [[paper]](https://arxiv.org/pdf/2210.00705.pdf) [[code]](https://github.com/atosystem/SpeechCLIP)
- Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese, 2022.11 [[paper]](https://arxiv.org/pdf/2211.01335.pdf) [[code]](https://github.com/OFA-Sys/Chinese-CLIP)
- Fine-tuned CLIP Models are Efficient Video Learners, 2022.12 [[paper]](https://arxiv.org/pdf/2212.03640.pdf) [[code]](https://github.com/muzairkhattak/ViFi-CLIP)
- Scaling Language-Image Pre-training via Masking, 2022.12 [[paper]](https://arxiv.org/pdf/2212.00794.pdf) [[code]](https://github.com/facebookresearch/flip)


### Text-to-3D Generation
- CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders, 2021.06 [[paper]](https://arxiv.org/pdf/2106.14843.pdf) [[code]](https://github.com/kvfrans/clipdraw/)
- CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation, 2021.10 [[paper]](https://arxiv.org/pdf/2110.02624.pdf) [[code]](https://github.com/AutodeskAILab/Clip-Forge)
- Text2Mesh: Text-Driven Neural Stylization for Meshes, 2021.12 [[paper]](https://arxiv.org/pdf/2112.03221.pdf) [[code]](https://github.com/threedle/text2mesh)
- CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields, 2021.12 [[paper]](https://arxiv.org/pdf/2112.05139.pdf) [[code]](https://github.com/cassiePython/CLIPNeRF)
- MotionCLIP: Exposing Human Motion Generation to CLIP Space, 2022.03 [[paper]](https://arxiv.org/pdf/2203.08063.pdf) [[code]](https://github.com/GuyTevet/MotionCLIP)
- AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars, 2022.05 [[paper]](https://arxiv.org/pdf/2205.08535.pdf) [[code]](https://github.com/hongfz16/AvatarCLIP)
- ClipFace: Text-guided Editing of Textured 3D Morphable Models, 2022.12 [[paper]](https://arxiv.org/pdf/2212.01406.pdf) [[code]](https://github.com/sanonymous22/ClipFace)


### Prompt Learning
- Learning to Prompt for Vision-Language Models, 2021.09 [[paper]](https://arxiv.org/pdf/2109.01134.pdf) [[code]](https://github.com/KaiyangZhou/CoOp)
- CLIP-Adapter: Better Vision-Language Models with Feature Adapters, 2021.10 [[paper]](https://arxiv.org/abs/2110.04544.pdf) [[code]](https://github.com/gaopengcuhk/CLIP-Adapter)
- Conditional Prompt Learning for Vision-Language Models, 2022.03 [[paper]](https://arxiv.org/pdf/2203.05557.pdf) [[code]](https://github.com/KaiyangZhou/CoOp)
- Learning to Compose Soft Prompts for Compositional Zero-Shot Learning, 2022.04 [[paper]](https://arxiv.org/pdf/2204.03574.pdf) [[code]](https://github.com/BatsResearch/csp)
- Prompt-aligned Gradient for Prompt Tuning, 2022.05 [[paper]](https://arxiv.org/pdf/2205.14865.pdf) [[code]](https://github.com/BeierZhu/Prompt-align)
- PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization, 2023.07 [[paper]](https://arxiv.org/pdf/2307.15199.pdf) [[web](https://promptstyler.github.io/)]

### Video Understanding
- VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding, 2021.09 [[paper]](https://arxiv.org/pdf/2109.14084.pdf) [[code]](https://github.com/pytorch/fairseq/tree/main/examples/MMPT)
- ActionCLIP: A New Paradigm for Video Action Recognition, 2021.09 [[paper]](https://arxiv.org/pdf/2109.08472.pdf) [[code]](https://github.com/sallymmx/actionclip)
- FitCLIP: Refining Large-Scale Pretrained Image-Text Models for Zero-Shot Video Understanding Tasks, 2022.03 [[paper]](https://arxiv.org/pdf/2203.13371.pdf) [[code]](https://github.com/bryant1410/fitclip)
- Frozen CLIP Models are Efficient Video Learners, 2022.08 [[paper]](https://arxiv.org/pdf/2208.03550.pdf) [[code]](https://github.com/OpenGVLab/efficient-video-recognition)
- Expanding Language-Image Pretrained Models for General Video Recognition, 2022.08 [[paper]](https://arxiv.org/abs/2208.02816) [[code]](https://github.com/microsoft/VideoX/tree/master/X-CLIP)
- Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization, 2022.10 [[paper]](https://arxiv.org/pdf/2210.12826.pdf) [[code]](https://github.com/pschaldenbrand/Text2Video)
- MovieCLIP: Visual Scene Recognition in Movies, 2022.10 [[paper]](https://arxiv.org/pdf/2210.11065v2.pdf) [[code]](https://github.com/usc-sail/mica-MovieCLIP)

### Image Captioning
- CLIP prefix captioning [[code]](https://github.com/rmokady/CLIP_prefix_caption)
- ClipCap: CLIP Prefix for Image Captioning, 2021.11 [[paper]](https://arxiv.org/pdf/2111.09734v1.pdf) [[code]](https://github.com/rmokady/CLIP_prefix_caption)
- CLIPScore: A Reference-free Evaluation Metric for Image Captioning, 2022.04 [[paper]](https://arxiv.org/pdf/2104.08718.pdf) [[code]](https://github.com/jmhessel/clipscore)
- Fine-grained Image Captioning with CLIP Reward, 2022.05 [[paper]](https://arxiv.org/pdf/2205.13115.pdf) [[code]](https://github.com/j-min/CLIP-Caption-Reward)
- Text-Only Training for Image Captioning using Noise-Injected CLIP, 2022.11 [[paper]](https://arxiv.org/pdf/2211.00575.pdf) [[code]](https://github.com/DavidHuji/CapDec)


### Image Editing 
- Image-based CLIP-Guided Essence Transfer, 2021.10 [[paper]](https://arxiv.org/pdf/2110.12427.pdf) [[code]](https://github.com/hila-chefer/TargetCLIP)
- HairCLIP: Design Your Hair by Text and Reference Image, 2021.12 [[paper]](https://arxiv.org/pdf/2112.05142.pdf) [[code]](https://github.com/wty-ustc/HairCLIP)
- CLIPstyler: Image Style Transfer with a Single Text Condition, 2021.12 [[paper]](https://arxiv.org/pdf/2112.00374.pdf) [[code]](https://github.com/paper11667/CLIPstyler)
- CLIPasso: Semantically-Aware Object Sketching, 2022.02 [[paper]](https://arxiv.org/pdf/2202.05822v2.pdf) [[code]](https://clipasso.github.io/clipasso/)
- CLIPDraw: Synthesize drawings to match a text prompt!, 2021.06 [[paper]](https://arxiv.org/pdf/2106.14843.pdf) [[code]](https://github.com/kvfrans/clipdraw)
- Towards Counterfactual Image Manipulation via CLIP, 2022.07 [[paper]](https://arxiv.org/pdf/2207.02812.pdf) [[code]](https://github.com/yingchen001/CF-CLIP)
- ClipCrop: Conditioned Cropping Driven by Vision-Language Model, 2022.11 [[paper]](https://arxiv.org/pdf/2211.11492.pdf) 
- CLIPascene: Scene Sketching with Different Types and Levels of Abstraction, 2022.11 [[paper]](https://arxiv.org/pdf/2211.17256.pdf) [[code]](https://clipascene.github.io/CLIPascene/)

### Image Segmentation
- Image Segmentation Using Text and Image Prompts, 2021.12 [[paper]](https://arxiv.org/pdf/2112.10003.pdf) [[code]](https://github.com/timojl/clipseg)
- Extract Free Dense Labels from CLIP, 2021.12 [[paper]](https://arxiv.org/pdf/2112.01071.pdf) [[code]](https://github.com/chongzhou96/MaskCLIP)
- CLIMS: Cross Language Image Matching for Weakly Supervised Semantic Segmentation, 2022.03 [[paper]](https://arxiv.org/pdf/2203.02668.pdf) [[code]](https://github.com/CVI-SZU/CLIMS)
- CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory, 2022.10 [[paper]](https://arxiv.org/pdf/2210.05663.pdf) [[code]](https://github.com/notmahi/clip-fields)
- Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP, 2022.10 [[paper]](https://arxiv.org/pdf/2210.04150.pdf) [[code]](https://github.com/facebookresearch/ov-seg)

### 3D Recognition
- PointCLIP: Point Cloud Understanding by CLIP, 2021.12 [[paper]](https://arxiv.org/pdf/2112.02413.pdf) [[code]](https://github.com/zrrskywalker/pointclip)
- MotionCLIP: Exposing Human Motion Generation to CLIP Space, 2022.03 [[paper]](https://arxiv.org/pdf/2203.08063.pdf) [[code]](https://github.com/GuyTevet/MotionCLIP)
- CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth Pre-training, 2022.10 [[paper]](https://arxiv.org/pdf/2210.01055.pdf) [[code]](https://github.com/tyhuang0428/CLIP2Point)
- LidarCLIP or: How I Learned to Talk to Point Clouds, 2022.12 [[paper]](https://arxiv.org/pdf/2212.06858.pdf)[[code]](https://github.com/atonderski/lidarclip)
- CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP, 2023.01 [[paper]](https://arxiv.org/pdf/2301.04926.pdf)

### Audio
- AudioCLIP: Extending CLIP to Image, Text and Audio, 2021.06 [[paper]](https://arxiv.org/pdf/2106.13043.pdf) [[code]](https://github.com/AndreyGuzhov/AudioCLIP)
- Wav2CLIP: Learning Robust Audio Representations from Clip, 2021.10 [[paper]](https://arxiv.org/pdf/2110.11499.pdf) [[code]](https://github.com/descriptinc/lyrebird-wav2clip)
- AVE-CLIP: AudioCLIP-based Multi-window Temporal Transformer for Audio Visual Event Localization, 2022.10 [[paper]](https://arxiv.org/pdf/2210.05060.pdf)

### Language Tasks
- CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment, 2022.03 [[paper]](https://arxiv.org/pdf/2203.07190v1.pdf)

### Object Navigation
- CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration, 2022.03 [[paper]](https://arxiv.org/pdf/2203.10421.pdf)

### Localization
- Adapting CLIP For Phrase Localization Without Further Training, 2022.04 [[paper]](https://arxiv.org/pdf/2204.03647.pdf) [[code]](https://github.com/pals-ttic/adapting-CLIP)

### Text Evaluation
- ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation, 2021.06 [[paper]](https://arxiv.org/pdf/2106.05970.pdf)

### GamePhysics
- CLIP meets GamePhysics: Towards bug identification in gameplay videos using zero-shot transfer learning, 2022.03 [[paper]](https://arxiv.org/pdf/2203.11096.pdf) [[code]](https://asgaardlab.github.io/CLIPxGamePhysics/)

## Distillation
### Object Detection
- Zero-Shot Detection via Vision and Language Knowledge Distillation, 2021.04 [[paper]](https://arxiv.org/pdf/2104.13921.pdf) [[code]](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild)

### Text-Video Retrieval
- TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval, 2021.04 [[paper]](https://arxiv.org/pdf/2104.08271.pdf) [[code]](https://github.com/albanie/collaborative-experts)
- Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers, 2021 [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Miech_Thinking_Fast_and_Slow_Efficient_Text-to-Visual_Retrieval_With_Transformers_CVPR_2021_paper.pdf)
- C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval, 2022.10 [[paper]](https://arxiv.org/pdf/2210.03625.pdf) [[code]](https://github.com/roudimit/c2kd)
- From Within to Between: Knowledge Distillation for Cross Modality Retrieval, 2022 [[paper]](https://openaccess.thecvf.com/content/ACCV2022/papers/Tran_From_Within_to_Between_Knowledge_Distillation_for_Cross_Modality_Retrieval_ACCV_2022_paper.pdf) [[code]](https://github.com/tqvinhcs/CrossKD)
- Learning Linguistic Association Towards Efficient Text-Video Retrieval, 2022 [[paper]](https://link.springer.com/chapter/10.1007/978-3-031-20059-5_15) [[code]](https://github.com/silenceFS/LINAS)
- CLIPPING: Distilling CLIP-Based Models with a Student Base for Video-Language Retrieval, 2022 [[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Pei_CLIPPING_Distilling_CLIP-Based_Models_With_a_Student_Base_for_Video-Language_CVPR_2023_paper.pdf)
- Dual Learning with Dynamic Knowledge Distillation for Partially Relevant Video Retrieval, 2023 [[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Dual_Learning_with_Dynamic_Knowledge_Distillation_for_Partially_Relevant_Video_ICCV_2023_paper.pdf)


## Others
- CLIP-Event: Connecting Text and Images with Event Structures, 2022.01 [[paper]](https://arxiv.org/abs/2201.05078) [[code]](https://github.com/limanling/clip-event)
- How Much Can CLIP Benefit Vision-and-Language Tasks?, 2022 [[paper]](https://openreview.net/forum?id=zf_Ll3HZWgy) [[code]](https://github.com/clip-vil/CLIP-ViL)
- Task Residual for Tuning Vision-Language Models, 2022.11 [[paper]](https://arxiv.org/pdf/2211.10277.pdf) [[code]](https://github.com/geekyutao/TaskRes)
- CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracywith ViT-B and ViT-L on ImageNet, 2022.12 [[paper]](https://arxiv.org/pdf/2212.06138v1.pdf) [[code]](https://github.com/lightdxy/ft-clip)
